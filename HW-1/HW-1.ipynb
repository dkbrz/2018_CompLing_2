{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "morph = MorphAnalyzer()\n",
    "stops = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>keywords</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>В среду состоялось отложенное заседание Совета...</td>\n",
       "      <td>[школа, образовательные стандарты, литература,...</td>\n",
       "      <td>Глава Минобрнауки считает, что в нездоровом аж...</td>\n",
       "      <td>Ольга Васильева обещала \"НГ\" не перегружать шк...</td>\n",
       "      <td>https://amp.ng.ru/?p=http://www.ng.ru/educatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хорошо, когда красота в глазах смотрящего живе...</td>\n",
       "      <td>[красота, законы]</td>\n",
       "      <td>О живительной пользе укорота при выборе между ...</td>\n",
       "      <td>У красоты собственные закон и воля</td>\n",
       "      <td>https://amp.ng.ru/?p=http://www.ng.ru/style/20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Когда-то Леонид Юзефович написал книгу о монго...</td>\n",
       "      <td>[юзефович, гражданская война, пепеляев, якутия]</td>\n",
       "      <td>Крепость из тел и призрак независимой Якутии</td>\n",
       "      <td>Апокалиптический бунт</td>\n",
       "      <td>https://amp.ng.ru/?p=http://www.ng.ru/zavisima...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  В среду состоялось отложенное заседание Совета...   \n",
       "1  Хорошо, когда красота в глазах смотрящего живе...   \n",
       "2  Когда-то Леонид Юзефович написал книгу о монго...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [школа, образовательные стандарты, литература,...   \n",
       "1                                  [красота, законы]   \n",
       "2    [юзефович, гражданская война, пепеляев, якутия]   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Глава Минобрнауки считает, что в нездоровом аж...   \n",
       "1  О живительной пользе укорота при выборе между ...   \n",
       "2       Крепость из тел и призрак независимой Якутии   \n",
       "\n",
       "                                               title  \\\n",
       "0  Ольга Васильева обещала \"НГ\" не перегружать шк...   \n",
       "1                 У красоты собственные закон и воля   \n",
       "2                              Апокалиптический бунт   \n",
       "\n",
       "                                                 url  \n",
       "0  https://amp.ng.ru/?p=http://www.ng.ru/educatio...  \n",
       "1  https://amp.ng.ru/?p=http://www.ng.ru/style/20...  \n",
       "2  https://amp.ng.ru/?p=http://www.ng.ru/zavisima...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_DATA = './data'\n",
    "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA)]\n",
    "data = pd.concat([pd.read_json(file, lines=True) for file in files], axis=0, ignore_index=True)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_kws, predicted_kws):\n",
    "    assert len(true_kws) == len(predicted_kws)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    jaccards = []\n",
    "    for i in range(len(true_kws)):\n",
    "        true_kw = set(true_kws[i])\n",
    "        predicted_kw = set(predicted_kws[i])\n",
    "        tp = len(true_kw & predicted_kw)\n",
    "        union = len(true_kw | predicted_kw)\n",
    "        fp = len(predicted_kw - true_kw)\n",
    "        fn = len(true_kw - predicted_kw)\n",
    "        if (tp+fp) == 0:prec = 0\n",
    "        else:prec = tp / (tp + fp)\n",
    "        if (tp+fn) == 0: rec = 0\n",
    "        else: rec = tp / (tp + fn)\n",
    "        if (prec+rec) == 0: f1 = 0\n",
    "        else: f1 = (2*(prec*rec))/(prec+rec)   \n",
    "        jac = tp / union\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "        jaccards.append(jac)\n",
    "    print('Precision - ', round(np.mean(precisions), 2))\n",
    "    print('Recall - ', round(np.mean(recalls), 2))\n",
    "    print('F1 - ', round(np.mean(f1s), 2))\n",
    "    print('Jaccard - ', round(np.mean(jaccards), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  1.0\n",
      "Recall -  1.0\n",
      "F1 -  1.0\n",
      "Jaccard -  1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punct = punctuation+'«»—…“”*№–'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Ухудшение\" 1. Что-то вроде Rake\n",
    "\n",
    "Выбираем последовательности из существительных и прилагательных так, что либо (adj)* (noun)* либо (noun)* , так как термины и ключевые слова утсроены так. Берем степень, частоту слова в целом, считаем коэффициент, для сочетания просто все складываем по составляющим. Так как наши ключевые слвоа все 1-2 слова длиной, берем только окончания таких сочетаний (последние 2 слова), иначе попадают в топ длинные последовательности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize2(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in wordpunct_tokenize(text.lower())]\n",
    "    words = [morph.parse(word)[0] if word and word not in stops else morph.parse('и')[0] for word in words ]\n",
    "    adj = True\n",
    "    add = False\n",
    "    words2 = []\n",
    "    for word in words:\n",
    "        if word.tag.POS == 'NOUN':\n",
    "            if add:\n",
    "                words2[-1].append(word.normal_form)\n",
    "                adj = False\n",
    "            else:\n",
    "                words2.append([word.normal_form])\n",
    "                add = True\n",
    "                adj = False\n",
    "        elif word.tag.POS == 'ADJF':\n",
    "            if adj and add:\n",
    "                    words2[-1].append(word.normal_form)\n",
    "            else:\n",
    "                    words2.append([word.normal_form])                \n",
    "            adj = True\n",
    "            add = True\n",
    "        else:\n",
    "            adj = True\n",
    "            add = False\n",
    "    return words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm2'] = data['content'].apply(normalize2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(text):\n",
    "    a = set()\n",
    "    for i in text:\n",
    "        a.update(i)\n",
    "    a = list(a)\n",
    "    matrix = np.zeros((len(a), len(a)))\n",
    "    for i in text:\n",
    "        for j in combinations(i[-3:], 2):\n",
    "            x = a.index(j[0])\n",
    "            y = a.index(j[1])\n",
    "            matrix[x, y] += 1\n",
    "            matrix[y, x] += 1\n",
    "        for j in i[-2:]:\n",
    "            x = a.index(j)\n",
    "            matrix[x,x] += 1\n",
    "    scores = np.zeros((len(a), 3))\n",
    "    norm = matrix.max()\n",
    "    for key, i in enumerate(a):\n",
    "        scores[key, 0] = sum(matrix[key,])\n",
    "        scores[key, 1] = matrix[key,key]\n",
    "        scores[key, 2] = scores[key, 0]\n",
    "    result = {}\n",
    "    for i in text:\n",
    "        k = i[-2:]\n",
    "        score = 0\n",
    "        for j in k:\n",
    "            key = a.index(j)\n",
    "            score += scores[key, 2]#*(scores[key, 1]/norm)\n",
    "        if len(k) == 2:\n",
    "            score = score * matrix[a.index(k[0]), a.index(k[1])] / norm\n",
    "        elif len(k) == 1:\n",
    "            score = score * matrix[a.index(k[0]), a.index(k[0])] / norm\n",
    "        if ' '.join(k) in result:\n",
    "            result[' '.join(k)] += score\n",
    "        else:\n",
    "            result[' '.join(k)] = score\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "исторический источник 153.0\n",
      "ольга васильев 117.0\n",
      "который 64.0\n",
      "стандарт 19.0\n",
      "образовательный стандарт 18.5\n",
      "предметный результат 15.0\n",
      "источник информация 15.0\n",
      "образовательный программа 13.0\n",
      "образовательный пространство 12.0\n",
      "редакция фгоса 11.0\n"
     ]
    }
   ],
   "source": [
    "d = get_scores(data.iloc[0]['content_norm2'])\n",
    "for i in sorted(d, key=d.get, reverse=True)[:10]:\n",
    "    print (i, d[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kw_r(text):\n",
    "    d = get_scores(text)\n",
    "    return list(sorted(d, key=d.get, reverse=True))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyowords = data['content_norm2'].apply(get_kw_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.14\n",
      "Recall -  0.14\n",
      "F1 -  0.13\n",
      "Jaccard -  0.08\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keyowords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает не очень по сравнению с tfidf, однако эта штука не зависит от общей выборки, полностью внутри текста - это иногда очень полезно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Ухудшение\" 2. Тот же псевдорэйк + tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прошлое + tfidf. Коэффициент умножается на 1 + tfidf, чтобы tfidf был как бы повышающим коэффициентом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_join(text):\n",
    "    text = [' '.join(i) for i in text]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm_str'] = data['content_norm2'].apply(double_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(data['content_norm_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {id2word[i]:i for i in id2word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_vectors = tfidf.transform(data['content_norm_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13828"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id['образовательный стандарт']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x27562 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 271 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(text):\n",
    "    a = set()\n",
    "    for i in text:\n",
    "        a.update(i)\n",
    "    a = list(a)\n",
    "    matrix = np.zeros((len(a), len(a)))\n",
    "    for i in text:\n",
    "        for j in combinations(i[-3:], 2):\n",
    "            x = a.index(j[0])\n",
    "            y = a.index(j[1])\n",
    "            matrix[x, y] += 1\n",
    "            matrix[y, x] += 1\n",
    "        for j in i[-2:]:\n",
    "            x = a.index(j)\n",
    "            matrix[x,x] += 1\n",
    "    scores = np.zeros((len(a), 3))\n",
    "    norm = matrix.max()\n",
    "    for key, i in enumerate(a):\n",
    "        scores[key, 0] = sum(matrix[key,])\n",
    "        scores[key, 1] = matrix[key,key]\n",
    "        scores[key, 2] = scores[key, 0]\n",
    "    result = {}\n",
    "    for i in text:\n",
    "        k = i[-2:]\n",
    "        score = 0\n",
    "        for j in k:\n",
    "            key = a.index(j)\n",
    "            score += scores[key, 2]#*(scores[key, 1]/norm)\n",
    "        #if len(k) == 2:\n",
    "        #    score = score * matrix[a.index(k[0]), a.index(k[1])] / norm\n",
    "        #elif len(k) == 1:\n",
    "        #    score = score * matrix[a.index(k[0]), a.index(k[0])] / norm\n",
    "        if ' '.join(k) in result:\n",
    "            result[' '.join(k)] += score\n",
    "        else:\n",
    "            result[' '.join(k)] = score\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "for key, value in enumerate(texts_vectors):\n",
    "    d = get_scores(data.iloc[key]['content_norm2'])\n",
    "    for i in d:\n",
    "        if i in word2id:\n",
    "            d[i] = d[i]*(1+value[0][0, word2id[i]])\n",
    "        else:\n",
    "            d[i] = 0\n",
    "    keywords.append(list(sorted(d, key=d.get, reverse=True))[:5])\n",
    "    #print (list(sorted(d, key=d.get, reverse=True))[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.13\n",
      "Recall -  0.13\n",
      "F1 -  0.12\n",
      "Jaccard -  0.07\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Ухудшение\" 3. То же самое, но другая метрика\n",
    "\n",
    "В этом случае наоборот, rake коэффициент является повышающим для tfidf. (1+log(rake)) * tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.normal_form for word in words if word.tag.POS in ('NOUN', 'ADJF')]\n",
    "\n",
    "    return words\n",
    "data['content_norm'] = data['content'].apply(normalize)\n",
    "data['content_norm_str'] = data['content_norm'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.12\n",
      "Recall -  0.23\n",
      "F1 -  0.15\n",
      "Jaccard -  0.09\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)\n",
    "tfidf.fit(data['content_norm_str'])\n",
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
    "texts_vectors = tfidf.transform(data['content_norm_str'])\n",
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-11:-1]] \n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize2(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in wordpunct_tokenize(text.lower())]\n",
    "    words = [morph.parse(word)[0] if word and word not in stops else morph.parse('и')[0] for word in words ]\n",
    "    adj = True\n",
    "    add = False\n",
    "    words2 = []\n",
    "    for word in words:\n",
    "        if word.tag.POS == 'NOUN':\n",
    "            if add:\n",
    "                words2[-1].append(word.normal_form)\n",
    "                adj = False\n",
    "            else:\n",
    "                words2.append([word.normal_form])\n",
    "                add = True\n",
    "                adj = False\n",
    "        elif word.tag.POS == 'ADJF':\n",
    "            if adj and add:\n",
    "                    words2[-1].append(word.normal_form)\n",
    "            else:\n",
    "                    words2.append([word.normal_form])                \n",
    "            adj = True\n",
    "            add = True\n",
    "        else:\n",
    "            adj = True\n",
    "            add = False\n",
    "    return words2\n",
    "data['content_norm2'] = data['content'].apply(normalize2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.17\n",
      "Recall -  0.17\n",
      "F1 -  0.16\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "keywords = []\n",
    "word2id = {id2word[i]:i for i in id2word}\n",
    "for key, value in enumerate(texts_vectors):\n",
    "    d = get_scores(data.iloc[key]['content_norm2'])\n",
    "    for i in d:\n",
    "        if i in word2id:\n",
    "            d[i] = (1+math.log(d[i]))*value[0][0, word2id[i]]\n",
    "        else:\n",
    "            d[i] = 0\n",
    "    keywords.append(list(sorted(d, key=d.get, reverse=True))[:5])\n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Улучшение 1. Препроцессинг\n",
    "\n",
    "Так как в ключевых словах встречаются не только существительные, но и прилагательные, нужно добавить и их. Плюс ключевые слова часто стоят во множественном числе и у прилагательных какой-то род, поэтому наши правильные ключевые слова могут просто не подходить по этим параметрам. Поэтому мы сохраняем род и число пирлагательных такими, какие они есть, просто переводим их и существительные в именительный падеж."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.inflect({'nomn'}).word for word in words if word.tag.POS in ('NOUN', 'ADJF')]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm'] = data['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm_str'] = data['content_norm'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)\n",
    "tfidf.fit(data['content_norm_str'])\n",
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_vectors = tfidf.transform(data['content_norm_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.2\n",
      "Recall -  0.2\n",
      "F1 -  0.19\n",
      "Jaccard -  0.11\n"
     ]
    }
   ],
   "source": [
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-6:-1]] \n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Улучшение 2. Постпроцессинг\n",
    "\n",
    "Ок, мы можем там терять термины, которые то во мн.ч, то в единственном. Можно считать все за одно, но запоминать, какие параметры (число, род, генитив) были часто у них в тексте. Найти ключевые в нормальной форме, а потом просклонять в нужную форму. Прилагательные: по роду и числу, существительные: по числу. Если существительное в основном в генитиве (типа совет федерации), то его в генитив. \n",
    "\n",
    "\n",
    "Сначала только для существительных, потом для существительных и прилагательных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.normal_form for word in words if word.tag.POS == 'NOUN']\n",
    "\n",
    "    return words\n",
    "\n",
    "data['content_norm'] = data['content'].apply(normalize)\n",
    "data['content_norm_str'] = data['content_norm'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)\n",
    "tfidf.fit(data['content_norm_str'])\n",
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
    "texts_vectors = tfidf.transform(data['content_norm_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-11:-1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_grammar(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words2 = {}\n",
    "    for word in words:\n",
    "        if word.tag.POS == 'NOUN':\n",
    "            lemma = word.normal_form\n",
    "            if lemma not in words2:\n",
    "                words2[lemma]={'sing':0, 'plur':0, 'gent':0, 'total':0}\n",
    "            words2[lemma][word.tag.number] += 1\n",
    "            if word.tag.case == 'gent':\n",
    "                words2[lemma]['gent'] += 1\n",
    "            words2[lemma]['total']+=1\n",
    "    return words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inflect_item(keyword, grammar):\n",
    "    keyword = [morph.parse(i)[0] for i in keyword.split()]\n",
    "    #print (keyword)\n",
    "    result = []\n",
    "    if len(keyword) == 2:\n",
    "        if keyword[0].tag.POS == 'NOUN' and keyword[1].tag.POS == 'NOUN':\n",
    "            try:\n",
    "                if grammar[keyword[0].normal_form]['plur'] > grammar[keyword[0].normal_form]['sing']:\n",
    "                    result.append(keyword[0].inflect({'plur'}).word)\n",
    "                else:\n",
    "                    result.append(keyword[0].normal_form)\n",
    "            except:\n",
    "                result.append(keyword[0].normal_form)\n",
    "            \n",
    "            try:\n",
    "                if grammar[keyword[1].normal_form]['gent'] > 0.4*grammar[keyword[1].normal_form]['total']:\n",
    "                    if grammar[keyword[1].normal_form]['plur'] > grammar[keyword[1].normal_form]['sing']:\n",
    "                        result.append(keyword[1].inflect({'gent', 'plur'}).word)\n",
    "                    else: result.append(keyword[1].inflect({'gent'}).word)\n",
    "            except:\n",
    "                result.append(keyword[1].normal_form)\n",
    "        \n",
    "        elif keyword[0].tag.POS == 'ADJF' and keyword[1].tag.POS == 'NOUN':\n",
    "            gender = keyword[1].tag.gender\n",
    "            try:\n",
    "                \n",
    "                if grammar[keyword[1].normal_form]['plur'] > grammar[keyword[1].normal_form]['sing']:\n",
    "                    number = 'plur'\n",
    "                    result.append(keyword[0].inflect({number}).word)\n",
    "                    result.append(keyword[1].inflect({number}).word)\n",
    "                else:\n",
    "                    number = 'sing'\n",
    "                    result.append(keyword[0].inflect({gender}).word)\n",
    "                    result.append(keyword[1].inflect({number}).word)\n",
    "            except:\n",
    "                result.append(keyword[0].inflect({gender}).word)\n",
    "                \n",
    "                result.append(keyword[1].normal_form)\n",
    "        else:\n",
    "            result = [i.normal_form for i in keyword]\n",
    "    elif len(keyword) == 1:\n",
    "        if keyword[0].tag.POS == 'NOUN':\n",
    "            if keyword[0].normal_form in grammar:\n",
    "                if grammar[keyword[0].normal_form]['plur'] > grammar[keyword[0].normal_form]['sing']:\n",
    "                    number = 'plur'\n",
    "                else: number = 'sing'\n",
    "                try:\n",
    "                    result.append(keyword[0].inflect({number}).word)\n",
    "                except:\n",
    "                    result.append(keyword[0].normal_form)\n",
    "            \n",
    "            else:\n",
    "                result = [i.normal_form for i in keyword]\n",
    "        else:\n",
    "            result = [i.normal_form for i in keyword]\n",
    "    else:\n",
    "        result = [i.normal_form for i in keyword]\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = []\n",
    "for key, value in enumerate(keywords):\n",
    "    kw.append([])\n",
    "    #print (keywords[key])\n",
    "    grammar = get_noun_grammar(data.iloc[key]['content'])\n",
    "    #print (grammar)\n",
    "    for item in value:\n",
    "        kw[-1].append(inflect_item(item, grammar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.14\n",
      "Recall -  0.26\n",
      "F1 -  0.18\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.2\n",
      "Recall -  0.19\n",
      "F1 -  0.19\n",
      "Jaccard -  0.12\n"
     ]
    }
   ],
   "source": [
    "kw = []\n",
    "for key, value in enumerate(keywords):\n",
    "    kw.append([])\n",
    "    #print (keywords[key])\n",
    "    grammar = get_noun_grammar(data.iloc[key]['content'])\n",
    "    #print (grammar)\n",
    "    for item in value[:5]:\n",
    "        kw[-1].append(inflect_item(item, grammar))\n",
    "evaluate(data['keywords'], kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это уже лучше бэйзлайна. Добавим прилагательные. Оставим топ-5 ключевых."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.normal_form for word in words if word.tag.POS in ('NOUN','ADJF')]\n",
    "\n",
    "    return words\n",
    "\n",
    "data['content_norm'] = data['content'].apply(normalize)\n",
    "data['content_norm_str'] = data['content_norm'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)\n",
    "tfidf.fit(data['content_norm_str'])\n",
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
    "texts_vectors = tfidf.transform(data['content_norm_str'])\n",
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-6:-1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.19\n",
      "Recall -  0.18\n",
      "F1 -  0.18\n",
      "Jaccard -  0.11\n"
     ]
    }
   ],
   "source": [
    "kw = []\n",
    "for key, value in enumerate(keywords):\n",
    "    kw.append([])\n",
    "    #print (keywords[key])\n",
    "    grammar = get_noun_grammar(data.iloc[key]['content'])\n",
    "    #print (grammar)\n",
    "    for item in value:\n",
    "        kw[-1].append(inflect_item(item, grammar))\n",
    "evaluate(data['keywords'], kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прилагательные на так часто встречаются, при этом ухудшают результат в среднем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Улучшение 3: NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имена встречаются среди ключевых слов, хочется их достать как-то по-нормальному и учесть их в разных формах (только фамилия, фамилия и имя). Для этого используем natasha.\n",
    "\n",
    "1. посмотреть только на имена (плохо, не везде они есть)\n",
    "2. посмотреть с базовой предобработкой (что если не нагроиождать сложную грамматическую информацию, а просто приклеить имена, насколько это повлияет на результат)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import PersonExtractor\n",
    "extractor_per = PersonExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names(text):\n",
    "    matches = extractor_per(text)\n",
    "    spans = [_.span for _ in matches]\n",
    "    facts = [_.fact.as_json for _ in matches]\n",
    "    result = []\n",
    "    for i in facts:\n",
    "        if 'last' in i['name']:\n",
    "            s = ' '.join(j for j in i['name'].values())\n",
    "            if len(s) > 3:\n",
    "                result.append(s)\n",
    "            s = i['name']['last']\n",
    "            if len(s) > 3:\n",
    "                result.append(s)\n",
    "    result = Counter(result)\n",
    "    result = [i for i in sorted(result, key=result.get, reverse=True) if result[i] > 2]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [get_names(i) for i in data['content'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.09\n",
      "Recall -  0.04\n",
      "F1 -  0.04\n",
      "Jaccard -  0.03\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.13\n",
      "Recall -  0.25\n",
      "F1 -  0.16\n",
      "Jaccard -  0.09\n"
     ]
    }
   ],
   "source": [
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.normal_form for word in words if word.tag.POS in ('NOUN',)]\n",
    "\n",
    "    return words\n",
    "\n",
    "data['content_norm'] = data['content'].apply(normalize)\n",
    "data['content_norm_str'] = data['content_norm'].apply(' '.join)\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)\n",
    "tfidf.fit(data['content_norm_str'])\n",
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
    "texts_vectors = tfidf.transform(data['content_norm_str'])\n",
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-11:-1]] \n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = []\n",
    "for key, value in enumerate(names):\n",
    "    k, ind = 0, 0\n",
    "    r = ['']+[i for i in value]\n",
    "    while len(r)<10 and ind != len(keywords[key]):\n",
    "        w = min(distance(j, keywords[key][ind]) for j in r)\n",
    "        if w > 2:\n",
    "            r.append(keywords[key][ind])\n",
    "            k += 1\n",
    "        ind += 1\n",
    "    new.append(r[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.13\n",
      "Recall -  0.25\n",
      "F1 -  0.16\n",
      "Jaccard -  0.09\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.14\n",
      "Recall -  0.24\n",
      "F1 -  0.17\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
